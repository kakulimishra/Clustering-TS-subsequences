library(dplyr)
library(igraph)
library(TSclust)
library(clusterSim)


dSAX=function (x, y, alpha, n, brkPoint) 
{
  
  x=strsplit(x,"")[[1]][1:3]
  y=strsplit(y,"")[[1]][1:3]
  
  w <- length(x)
  myLetters<-letters[1:26]
  
  SAX.breakpoints.table <- function (n) {
    qnorm(0:n/n)
  }
  
  symb <- SAX.breakpoints.table(alpha)
  d <- 0
  
  x1<- match(x, myLetters)
  y1<-match(y, myLetters)
  
  for (i in 1:w) {
    xi <- x1[i]
    yi <- y1[i]
    
    if (abs(xi - yi) > 1 ) {
      d <- d + (symb[max(xi, yi)] - symb[min(xi, yi)+1])^2
    }
    
    else if (abs(xi-yi)==1 )
    {
      brkPoint.sub = brkPoint[which(brkPoint$feature==i),]
      d <- d + (brkPoint.sub$val[brkPoint.sub$alpha==max(xi,yi)] - brkPoint.sub$val[brkPoint.sub$alpha==min(xi,yi)])^2
    }
    
  }
  sqrt((n/w) * d)
}


graphConstruct=function(edge.list)
{
  mainGraph <- graph.data.frame(edge.list[,c("from","to")], directed = T
  )
  l <- layout_with_kk(mainGraph)
  
  # plot(mainGraph, layout=l, vertex.size=3,
  #      vertex.color="red", edge.arrow.size=0.2,vertex.cex=0.55,vertex.label.dist=1,
  #      vertex.label.color="blue4",asp=0,vertex.label.font=12,edge.width=2,vertex.size2=12)
  
  return (mainGraph)
}



compute_edgeWeights=function(edgeList,numAlpha,window,brkPoint)
{
  edgeWeights=apply(edgeList,1,function(f)
  {
    mean_dist= dSAX(f[1],f[2], numAlpha, window, brkPoint)
    
  })
  edgeWeights
}




from_to=function(temp)
{
  
  #Create a dataframe that has "from" and "to" nodes, i.e, vertices onnected by edges.
  graphEdges=NULL
  for (j in 1:(length(temp)-1))
  {
    graphEdges=rbind(graphEdges,cbind("from"=as.character(temp[[j]]),"to"=as.character(temp[[j+1]])))
    
  }
  return (data.frame(graphEdges,stringsAsFactors = F))
}




discover_edges= function(symbolicRep)
{

  edge.list= NULL
  for (i in 1:nrow(symbolicRep))
  {
    
    temp= (symbolicRep[i,])
    edge.list= data.frame(rbind(edge.list,x=from_to(temp)),row.names = NULL)
    temp= NULL
    
  }
  colnames(edge.list)= c("from","to")
  
  ##Remove duplicate edges
  edge.list= edge.list[!duplicated(edge.list),]
  
  edge.list = data.frame(edge.list[!(edge.list$from==edge.list$to),], stringsAsFactors = F, row.names = NULL)
  
  return (edge.list)
}


calls_to_clustering_techniques=function(fileToRead, dataSet)
{
  clusterQuality=NULL
  winLength = c(1, 2, 4, 6, 8, 12, 24) 
  for (z in winLength)
  {
    
    window= z*2
    print (window)
    dta.norm=(fileToRead-mean(unlist(fileToRead)))/sd(unlist(fileToRead))
    
    name_sym = paste0(dataSet,"_sym_", "win", window, ".csv")
    symbolicRep = read.csv(file.path(getwd(), "Results", "Symbolic representation",name_sym),header=T)
    
    numAlpha = read.csv(file.path(getwd(), "Results", "Proposed", paste0(dataSet,"_numAlpha",".csv")),header=T)  
    numAlpha = numAlpha[which (numAlpha$window == window),]$alpha
    
    name_brkPoint=paste0(dataSet,"_brkPoint", "_win",window, ".csv")
    brkPoint = read.csv( file.path(getwd(), "Results", "Proposed",name_brkPoint),header=T)
    
    
    edgeList= discover_edges(symbolicRep)
    
    ##edgeList stores the vertex ids of the edges
    edgeWeights= round(compute_edgeWeights(edgeList,numAlpha,window,brkPoint),3)
    
    mainGraph= graphConstruct(edgeList)
    E(mainGraph)$weight = edgeWeights
    E(mainGraph)$id=seq(1, length(E(mainGraph)))
    
    ##Min-max normalization of weights, so that they range from 0 to 1
    E(mainGraph)$weight = round ((E(mainGraph)$weight - min(E(mainGraph)$weight))/ (max(E(mainGraph)$weight)- min(E(mainGraph)$weight)),4)
    
    weighted_edges= data.frame(cbind(get.edgelist(mainGraph),"weights"=E(mainGraph)$weight), row.names = NULL, stringsAsFactors = F)
    colnames(weighted_edges)= c("from","to","weights")
    
    
    ###*****************************************************************************************************************************************
    #Proposed quasi clique clustering is called
    for (beta in seq(0.3, 0.8, 0.05))
    {
      start= Sys.time()
      print (beta)
      clusterLabels = quasiClique_clustering(mainGraph, weighted_edges, beta)
      filename_labels = paste0(dataSet,"_clusterLabels_", "win", window, "_beta", beta, ".csv")
      write.csv(data.frame(clusterLabels), file.path(getwd(), "Results", "Proposed", filename_labels),row.names=F)
      cq = clusterQuality(clusterLabels, mainGraph)
      write.table(cbind(cq[[1]],cq[[2]], cq[[3]], beta, window, "Proposed"), file.path(getwd(), "Results", "Proposed", paste0(dataSet,"_intraclustMeasure.csv")),
                    append=T, row.names = F, col.names = F, sep=",")
      print (Sys.time()- start)
    }
    #*******************************************************************************************************************************************
    
    ##other clustering techniques for comparison
    clusterlabels = mst_eb_clustering(as.undirected(maingraph),edge.list)
    filename_labels =paste0(dataset,"_clusterlabels_", "mst-eb_","win", window,".csv")
    write.csv(data.frame(clusterLabels), file.path(getwd(), "Results", "MST-EB", filename_labels), row.names=F)
    cq = clusterQuality(clusterLabels, mainGraph)
    write.table(cbind(cq[[1]],cq[[2]], cq[[3]], NA, window, "MST-EB" ), file.path(getwd(), "Results", "Proposed", paste0(dataSet,"_intraclustMeasure.csv")),
                append=T, row.names = F, col.names = F, sep=",")
    
    
    
    clusterlabels = mst_vb_clustering(as.undirected(maingraph),edge.list)
    filename_labels =paste0(dataset,"_clusterlabels_", "mst-vb_","win", window,".csv")
    write.csv(data.frame(clusterlabels), file.path(getwd(), "results", "mst-vb", filename_labels),row.names=f)
    cq = clusterQuality(clusterLabels, mainGraph)
    write.table(cbind(cq[[1]],cq[[2]], cq[[3]], NA, window, "MST-VB" ), file.path(getwd(), "Results", "Proposed", paste0(dataSet,"_intraclustMeasure.csv")),
                append=T, row.names = F, col.names = F, sep=",")
    
    
    clusterlabels= multilevel_clustering(as.undirected(maingraph),edge.list)
    filename_labels =paste0(dataset,"_clusterlabels_", "multilevel_","win", window,".csv")
    write.csv(data.frame(clusterlabels), file.path(getwd(), "results", "multilevel", filename_labels),row.names=f)
    cq = clusterQuality(clusterLabels, mainGraph)
    write.table(cbind(cq[[1]],cq[[2]], cq[[3]], NA, window, "Multilevel" ), file.path(getwd(), "Results", "Proposed", paste0(dataSet,"_intraclustMeasure.csv")),
                append=T, row.names = F, col.names = F, sep=",")
    
    
    clusterlabels=infomap_clustering(maingraph, weighted_edges)
    filename_labels =paste0(dataset,"_clusterlabels_", "infomap_","win", window,".csv")
    write.csv(data.frame(clusterlabels), file.path(getwd(), "results", "infomap", filename_labels),row.names=f)
    cq = clusterQuality(clusterLabels, mainGraph)
    write.table(cbind(cq[[1]],cq[[2]],cq[[3]], NA, window, "Infomap" ), file.path(getwd(), "Results", "Proposed", paste0(dataSet,"_intraclustMeasure.csv")),
                append=T, row.names = F, col.names = F, sep=",")
    
  }
  
}


dataSet=c("London", "Ausgrid", "Stockmarket", "Webtraffic", "IRISH")

for (p in dataSet)
{
  data_nm= p
  dataFile= read.csv(file.path(getwd(),paste0(data_nm,"Dataset.csv")),header=T)
  print (data_nm)
  if (data_nm=="Ausgrid" | data_nm == "Stockmarket" | data_nm == "Webtraffic" | data_nm == "IRISH")
    dataFile=dataFile[,-c(1,2)]
  else
    dataFile = dataFile[,-c(1,2,3)]
  calls_to_clustering_techniques(dataFile, data_nm)
  
}
